{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "neuralnets.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-30S9GMdhsqx",
        "outputId": "691f7e29-bbb2-46f4-8730-94c586c47cb5"
      },
      "source": [
        "#Import libraries\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms, datasets\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import csv\n",
        "import librosa\n",
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import pathlib\n",
        "import csv\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "import keras\n",
        "from keras import layers\n",
        "from keras import layers\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import pandas as pd\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pVWeskdCh3pC",
        "outputId": "ed39b744-6629-4eb1-ad82-db83f6acbef7"
      },
      "source": [
        "#5 birbs!!\n",
        "\n",
        "header = 'filename chroma_stft rmse spectral_centroid spectral_bandwidth rolloff zero_crossing_rate'\n",
        "for i in range(1, 21):\n",
        "    header += f' mfcc{i}'\n",
        "header += ' label'\n",
        "header = header.split()\n",
        "\n",
        "file = open('dataset.csv', 'w', newline='')\n",
        "with file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow(header)\n",
        "birbs = ['aldfly', 'ameavo', 'amebit', 'amecro', 'amegfi']\n",
        "for birb in birbs:\n",
        "    print(birb)\n",
        "    for filename in os.listdir(f'/content/drive/My Drive/MLSP/train_audio_5/{birb}'):\n",
        "        songname = f'/content/drive/My Drive/MLSP/train_audio_5/{birb}/{filename}'\n",
        "        y, sr = librosa.load(songname, mono=True, duration=15)\n",
        "        rmse = librosa.feature.rmse(y=y)\n",
        "        chroma_stft = librosa.feature.chroma_stft(y=y, sr=sr)\n",
        "        spec_cent = librosa.feature.spectral_centroid(y=y, sr=sr)\n",
        "        spec_bw = librosa.feature.spectral_bandwidth(y=y, sr=sr)\n",
        "        rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)\n",
        "        zcr = librosa.feature.zero_crossing_rate(y)\n",
        "        mfcc = librosa.feature.mfcc(y=y, sr=sr)\n",
        "        to_append = f'{filename} {np.mean(chroma_stft)} {np.mean(rmse)} {np.mean(spec_cent)} {np.mean(spec_bw)} {np.mean(rolloff)} {np.mean(zcr)}'    \n",
        "        for e in mfcc:\n",
        "            to_append += f' {np.mean(e)}'\n",
        "        to_append += f' {birb}'\n",
        "        file = open('dataset.csv', 'a', newline='')\n",
        "        with file:\n",
        "            writer = csv.writer(file)\n",
        "            writer.writerow(to_append.split())\n",
        "\n"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "aldfly\n",
            "ameavo\n",
            "amebit\n",
            "amecro\n",
            "amegfi\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KaTwZIE8rnvy",
        "outputId": "f81aae0f-3d38-43a9-dd67-d561c279d05f"
      },
      "source": [
        "os.listdir(f'/content/drive/My Drive/MLSP/train_audio_5/')"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['aldfly', 'amegfi', 'amecro', 'amebit', 'ameavo']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDYDIwADnmIs"
      },
      "source": [
        "data = pd.read_csv('dataset.csv')\n",
        "data.head()\n",
        "data = data.drop(['filename'],axis=1)\n",
        "genre_list = data.iloc[:, -1]\n",
        "encoder = LabelEncoder()\n",
        "y = encoder.fit_transform(genre_list)\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(np.array(data.iloc[:, :-1], dtype = float))"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVP3h4W7rNjy"
      },
      "source": [
        "data.to_csv('dataset.csv')\n",
        "!cp dataset.csv \"drive/My Drive/\"\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evK7vIPPnnsG"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(layers.Dense(256, activation='relu', input_shape=(X_train.shape[1],)))\n",
        "model.add(layers.Dense(128, activation='relu'))\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "model.add(layers.Dense(10, activation='softmax'))\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qPNPnAsknvBQ",
        "outputId": "ca49b6f6-a6e0-4618-d8f9-c4618450d6d9"
      },
      "source": [
        "classifier = model.fit(X_train,\n",
        "                    y_train,\n",
        "                    epochs=20,\n",
        "                    batch_size=50)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 2.1153 - accuracy: 0.3049\n",
            "Epoch 2/20\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 1.6004 - accuracy: 0.6197\n",
            "Epoch 3/20\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 1.2656 - accuracy: 0.6754\n",
            "Epoch 4/20\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 1.0036 - accuracy: 0.6885\n",
            "Epoch 5/20\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 0.7650 - accuracy: 0.7443\n",
            "Epoch 6/20\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.6019 - accuracy: 0.8361\n",
            "Epoch 7/20\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 0.4623 - accuracy: 0.8787\n",
            "Epoch 8/20\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 0.3563 - accuracy: 0.8918\n",
            "Epoch 9/20\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 0.2877 - accuracy: 0.9246\n",
            "Epoch 10/20\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 0.2184 - accuracy: 0.9475\n",
            "Epoch 11/20\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 0.2033 - accuracy: 0.9639\n",
            "Epoch 12/20\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 0.1518 - accuracy: 0.9738\n",
            "Epoch 13/20\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 0.1273 - accuracy: 0.9672\n",
            "Epoch 14/20\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 0.1073 - accuracy: 0.9836\n",
            "Epoch 15/20\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 0.0841 - accuracy: 0.9902\n",
            "Epoch 16/20\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 0.0797 - accuracy: 0.9869\n",
            "Epoch 17/20\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 0.0650 - accuracy: 0.9902\n",
            "Epoch 18/20\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 0.0563 - accuracy: 0.9967\n",
            "Epoch 19/20\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 0.0539 - accuracy: 0.9967\n",
            "Epoch 20/20\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 0.0498 - accuracy: 0.9902\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_EpuFdgL5Moo",
        "outputId": "4edbb4cd-81ed-4454-fe84-1849526baa7e"
      },
      "source": [
        "model.evaluate(x=X_test, y=y_test)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3/3 [==============================] - 0s 2ms/step - loss: 0.3882 - accuracy: 0.8701\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.38820430636405945, 0.8701298832893372]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fA7DnsvxreOC"
      },
      "source": [
        "# don't run this stuff and the rest from here \n",
        "\n",
        "header = 'filename chroma_stft rmse spectral_centroid spectral_bandwidth rolloff zero_crossing_rate'\n",
        "for i in range(1, 21):\n",
        "    header += f' mfcc{i}'\n",
        "header += ' label'\n",
        "header = header.split()\n",
        "\n",
        "file = open('train_dataset.csv', 'w', newline='')\n",
        "with file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow(header)\n",
        "birbs = os.listdir(f'/content/drive/My Drive/MLSP/train_audio/')[:20]\n",
        "for birb in birbs:\n",
        "    print(birb)\n",
        "    for filename in os.listdir(f'/content/drive/My Drive/MLSP/train_audio/{birb}'):\n",
        "        songname = f'/content/drive/My Drive/MLSP/train_audio/{birb}/{filename}'\n",
        "        y, sr = librosa.load(songname, mono=True, duration=15)\n",
        "        rmse = librosa.feature.rmse(y=y)\n",
        "        chroma_stft = librosa.feature.chroma_stft(y=y, sr=sr)\n",
        "        spec_cent = librosa.feature.spectral_centroid(y=y, sr=sr)\n",
        "        spec_bw = librosa.feature.spectral_bandwidth(y=y, sr=sr)\n",
        "        rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)\n",
        "        zcr = librosa.feature.zero_crossing_rate(y)\n",
        "        mfcc = librosa.feature.mfcc(y=y, sr=sr)\n",
        "        to_append = f'{filename} {np.mean(chroma_stft)} {np.mean(rmse)} {np.mean(spec_cent)} {np.mean(spec_bw)} {np.mean(rolloff)} {np.mean(zcr)}'    \n",
        "        for e in mfcc:\n",
        "            to_append += f' {np.mean(e)}'\n",
        "        to_append += f' {birb}'\n",
        "        file = open('train_dataset.csv', 'a', newline='')\n",
        "        with file:\n",
        "            writer = csv.writer(file)\n",
        "            writer.writerow(to_append.split())\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NsBZQqkM1E80"
      },
      "source": [
        "data = pd.read_csv('train_dataset.csv')\n",
        "data.head()\n",
        "data = data.drop(['filename'],axis=1)\n",
        "genre_list = data.iloc[:, -1]\n",
        "encoder = LabelEncoder()\n",
        "y = encoder.fit_transform(genre_list)\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(np.array(data.iloc[:, :-1], dtype = float))\n",
        "\n",
        "data.to_csv('train_dataset.csv')\n",
        "!cp train_dataset.csv \"drive/My Drive/\""
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bPzU2nj91KhW"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "classifier = model.fit(X_train,\n",
        "                    y_train,\n",
        "                    epochs=20,\n",
        "                    batch_size=50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1_q_9_xKeKP"
      },
      "source": [
        "model.evaluate(x=X_test, y=y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3GVgbcz71eJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}